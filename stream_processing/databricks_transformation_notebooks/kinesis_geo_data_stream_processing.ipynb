{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AWS access key and secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1914d944-1d7f-4341-a9fc-882c332edbdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, DoubleType\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc65b04d-4a8b-412e-b89c-a6cd1b7ecb13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimiter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    "    .option(\"header\", first_row_is_header)\\\n",
    "    .option(\"sep\", delimiter)\\\n",
    "    .load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f470330b-d413-47e0-8b63-1d3972727a75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(F.col(\"User name\")==\"databricks-user\").select('Access key ID').collect()[0]['Access key ID']\n",
    "\n",
    "SECRET_KEY = aws_keys_df.where(F.col(\"User name\")==\"databricks-user\").select(\"Secret access key\").collect()[0][\"Secret access key\"]\n",
    "\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c3275e-6f52-4d04-89fc-dc540c6f0145",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the output and checkpoint paths for the cleaned user data\n",
    "outputPath = '/mnt/pinterest_data/test_streaming_delta_tables/0e3bbd435bfb_geo_table'\n",
    "checkpointPath = '/mnt/pinterest_data/test_streaming_delta_tables/checkpoints/geo'\n",
    "\n",
    "# define partition key\n",
    "partition_key = \"test_geo\"\n",
    "\n",
    "# define aws access variables and a dataframe to read from kinesis stream\n",
    "awsAccessKeyId = ACCESS_KEY\n",
    "awsSecretKey = SECRET_KEY\n",
    "kinesisStreamName = \"streaming-0e3bbd435bfb-geo\"\n",
    "kinesisRegion = \"us-east-1\"\n",
    "df = (spark.readStream\n",
    "    .format(\"kinesis\") \n",
    "    .option(\"streamName\", kinesisStreamName)\n",
    "    .option(\"region\", kinesisRegion)\n",
    "    .option(\"initialPosition\", \"LATEST\")\n",
    "    .option(\"format\", \"json\")\n",
    "    .option(\"awsAccessKey\", awsAccessKeyId)\n",
    "    .option(\"awsSecretKey\", awsSecretKey)\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"minFetchPeriod\", \"200ms\")\n",
    "    .load())\n",
    "\n",
    "# schema for the data\n",
    "schema = StructType([\n",
    "    StructField(\"ind\", LongType()),\n",
    "    StructField(\"timestamp\", StringType()),\n",
    "    StructField(\"latitude\", DoubleType()),\n",
    "    StructField(\"longitude\", DoubleType()),\n",
    "    StructField(\"country\", StringType())\n",
    "])\n",
    "\n",
    "# transformations\n",
    "transformed_df = (\n",
    "    # filter data with required shardId\n",
    "    df.filter(F.col(\"partitionKey\") == partition_key)\n",
    "\n",
    "        # decode the data column\n",
    "        .withColumn(\n",
    "            \"decoded_data\",\n",
    "            F.unbase64(\n",
    "                F.col(\"data\")\n",
    "            ).cast(\"string\")\n",
    "        )\n",
    "\n",
    "        # Use from_json to parse the JSON string in decoded_data and apply the schema\n",
    "        .withColumn(\"parsed_data\", F.from_json(F.col(\"decoded_data\"), schema))\n",
    "        \n",
    "        # select the individual fields from the parsed_data column\n",
    "        .select(\n",
    "            F.col(\"parsed_data.ind\"),\n",
    "            F.col(\"parsed_data.timestamp\"),\n",
    "            F.col(\"parsed_data.latitude\"),\n",
    "            F.col(\"parsed_data.longitude\"),\n",
    "            F.col(\"parsed_data.country\"),\n",
    "        )\n",
    "\n",
    "        # create a new column coordinates with latitude and longitude\n",
    "        .withColumn(\n",
    "            \"coordinates\",\n",
    "            F.struct(\n",
    "                F.col(\"latitude\"),\n",
    "                F.col(\"longitude\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # drop the latitude and longitude columns\n",
    "        .drop(\"latitude\", \"longitude\")\n",
    "\n",
    "        # convert \"timestamp\" column from string to timestamp\n",
    "        .withColumn(\n",
    "            \"timestamp\",\n",
    "            F.to_timestamp(\n",
    "                F.col(\"timestamp\")\n",
    "            ).cast(\"timestamp\")\n",
    "        )\n",
    "\n",
    "        # reorder the columns\n",
    "        .select([\"ind\", \"country\", \"coordinates\", \"timestamp\"])\n",
    ")\n",
    "\n",
    "# write the stream to a delta table\n",
    "query = (\n",
    "    transformed_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpointPath)\n",
    "    .start(outputPath)\n",
    ")\n",
    "\n",
    "# keep the stream running\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "kinesis_geo_data_stream_processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
