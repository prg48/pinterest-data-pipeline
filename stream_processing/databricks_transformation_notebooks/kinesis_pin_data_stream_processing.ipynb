{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load AWS access key and secret key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1914d944-1d7f-4341-a9fc-882c332edbdd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc65b04d-4a8b-412e-b89c-a6cd1b7ecb13",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Specify file type to be csv\n",
    "file_type = \"csv\"\n",
    "# Indicates file has first row as the header\n",
    "first_row_is_header = \"true\"\n",
    "# Indicates file has comma as the delimiter\n",
    "delimiter = \",\"\n",
    "# Read the CSV file to spark dataframe\n",
    "aws_keys_df = spark.read.format(file_type)\\\n",
    "    .option(\"header\", first_row_is_header)\\\n",
    "    .option(\"sep\", delimiter)\\\n",
    "    .load(\"/FileStore/tables/authentication_credentials.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f470330b-d413-47e0-8b63-1d3972727a75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the AWS access key and secret key from the spark dataframe\n",
    "ACCESS_KEY = aws_keys_df.where(F.col(\"User name\")==\"databricks-user\").select('Access key ID').collect()[0]['Access key ID']\n",
    "\n",
    "SECRET_KEY = aws_keys_df.where(F.col(\"User name\")==\"databricks-user\").select(\"Secret access key\").collect()[0][\"Secret access key\"]\n",
    "\n",
    "# Encode the secret key\n",
    "ENCODED_SECRET_KEY = urllib.parse.quote(string=SECRET_KEY, safe=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stream transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71c3275e-6f52-4d04-89fc-dc540c6f0145",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Menlo\", \"Monaco\", \"Consolas\", \"Ubuntu Mono\", \"Source Code Pro\", monospace;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Cancelled",
       "errorTraceType": "html",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the output and checkpoint paths for the cleaned user data\n",
    "outputPath = '/mnt/pinterest_data/test_streaming_delta_tables/0e3bbd435bfb_pin_table'\n",
    "checkpointPath = '/mnt/pinterest_data/test_streaming_delta_tables/checkpoints/pin'\n",
    "\n",
    "# define partition key\n",
    "partition_key = \"test_pin\"\n",
    "\n",
    "# define aws access variables and a dataframe to read from kinesis stream\n",
    "awsAccessKeyId = ACCESS_KEY\n",
    "awsSecretKey = SECRET_KEY\n",
    "kinesisStreamName = \"streaming-0e3bbd435bfb-pin\"\n",
    "kinesisRegion = \"us-east-1\"\n",
    "df = (spark.readStream\n",
    "    .format(\"kinesis\") \n",
    "    .option(\"streamName\", kinesisStreamName)\n",
    "    .option(\"region\", kinesisRegion)\n",
    "    .option(\"initialPosition\", \"LATEST\")\n",
    "    .option(\"format\", \"json\")\n",
    "    .option(\"awsAccessKey\", awsAccessKeyId)\n",
    "    .option(\"awsSecretKey\", awsSecretKey)\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"minFetchPeriod\", \"200ms\")\n",
    "    .load())\n",
    "\n",
    "# schema for the data\n",
    "schema = StructType([\n",
    "    StructField(\"index\", LongType()),\n",
    "    StructField(\"unique_id\", StringType()),\n",
    "    StructField(\"title\", StringType()),\n",
    "    StructField(\"description\", StringType()),\n",
    "    StructField(\"poster_name\", StringType()),\n",
    "    StructField(\"follower_count\", StringType()),\n",
    "    StructField(\"tag_list\", StringType()),\n",
    "    StructField(\"is_image_or_video\", StringType()),\n",
    "    StructField(\"image_src\", StringType()),\n",
    "    StructField(\"downloaded\", LongType()),\n",
    "    StructField(\"save_location\", StringType()),\n",
    "    StructField(\"category\", StringType())\n",
    "])\n",
    "\n",
    "# define a udf to convert numerical abbreviation to string numeric form\n",
    "@F.udf(returnType=StringType())\n",
    "def convert_numeric_abb_to_str_numeric_representation(value: str) -> str:\n",
    "    \"\"\"\n",
    "    converts value in numberical abbreviation form to string numeric form\n",
    "\n",
    "    Args:\n",
    "        value (str): value in numerical abbreviation form\n",
    "\n",
    "    Returns:\n",
    "        str: string numeric form of value\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if value == None:\n",
    "            return None\n",
    "        elif value[-1] == \"M\":\n",
    "            num_part = value[:-1]\n",
    "            return num_part + \"000000\"\n",
    "        elif value[-1] == \"k\":\n",
    "            num_part = value[:-1]\n",
    "            return num_part + '000'\n",
    "        else:\n",
    "            return value\n",
    "    except Exception as e:\n",
    "        return \"An exception occured!\"\n",
    "\n",
    "# transformations\n",
    "transformed_df = (\n",
    "    # filter data with required shardId\n",
    "    df.filter(F.col(\"partitionKey\") == partition_key)\n",
    "\n",
    "        # decode the data column\n",
    "        .withColumn(\n",
    "            \"decoded_data\",\n",
    "            F.unbase64(\n",
    "                F.col(\"data\")\n",
    "            ).cast(\"string\")\n",
    "        )\n",
    "\n",
    "        # Use from_json to parse the JSON string in decoded_data and apply the schema\n",
    "        .withColumn(\"parsed_data\", F.from_json(F.col(\"decoded_data\"), schema))\n",
    "        \n",
    "        # select the individual fields from the parsed_data column\n",
    "        .select(\n",
    "            F.col(\"parsed_data.index\"),\n",
    "            F.col(\"parsed_data.unique_id\"),\n",
    "            F.col(\"parsed_data.title\"),\n",
    "            F.col(\"parsed_data.description\"),\n",
    "            F.col(\"parsed_data.poster_name\"),\n",
    "            F.col(\"parsed_data.follower_count\"),\n",
    "            F.col(\"parsed_data.tag_list\"),\n",
    "            F.col(\"parsed_data.is_image_or_video\"),\n",
    "            F.col(\"parsed_data.image_src\"),\n",
    "            F.col(\"parsed_data.downloaded\"),\n",
    "            F.col(\"parsed_data.save_location\"),\n",
    "            F.col(\"parsed_data.category\"),\n",
    "        )\n",
    ")\n",
    "\n",
    "# Replace empty strings with null\n",
    "for column in transformed_df.columns:\n",
    "    transformed_df = transformed_df.withColumn(\n",
    "        column,\n",
    "        F.when(\n",
    "            F.col(column) == \"\", None\n",
    "        ).otherwise(F.col(column))\n",
    "    )\n",
    "\n",
    "# define irrelevant values for the description column\n",
    "irrelevant_data = [\"No description available Story format\", \"No description available\"]\n",
    "\n",
    "# other transformations\n",
    "transformed_df = (\n",
    "    # change the irrelevant values in description column to null\n",
    "    transformed_df.withColumn(\n",
    "        \"description\",\n",
    "        F.when(\n",
    "            (F.col(\"description\") == irrelevant_data[0]) | (F.col(\"description\") == irrelevant_data[1]), None\n",
    "        ).otherwise(F.col(\"description\"))\n",
    "    )\n",
    "\n",
    "    # change 'User Info Error' values to null in 'follower_count' column\n",
    "    .withColumn(\n",
    "        \"follower_count\",\n",
    "        F.when(F.col(\"follower_count\") == \"User Info Error\", None).otherwise(F.col(\"follower_count\"))\n",
    "    )\n",
    "\n",
    "    # change 'Image src error' values to null in 'image_src' column\n",
    "    .withColumn(\n",
    "        \"image_src\",\n",
    "        F.when(F.col(\"image_src\") == 'Image src error', None).otherwise(F.col(\"image_src\"))\n",
    "    )\n",
    "\n",
    "    # change 'User Info Error' values to null in 'poster_name' column\n",
    "    .withColumn(\n",
    "        \"poster_name\",\n",
    "        F.when(F.col(\"poster_name\") == 'User Info Error', None).otherwise(F.col(\"poster_name\"))\n",
    "    )\n",
    "\n",
    "    # change \"N,o,,T,a,g,s,,A,v,a,i,l,a,b,l,e\" value to null in 'tag_list'\n",
    "    .withColumn(\n",
    "        \"tag_list\",\n",
    "        F.when(F.col(\"tag_list\") == \"N,o, ,T,a,g,s, ,A,v,a,i,l,a,b,l,e\", None).otherwise(F.col(\"tag_list\"))\n",
    "    )\n",
    "\n",
    "    # change \"No Title Data Available\" value to null in 'title' column\n",
    "    .withColumn(\n",
    "        \"title\",\n",
    "        F.when(F.col(\"title\") == \"No Title Data Available\", None).otherwise(F.col(\"title\"))\n",
    "    )\n",
    "\n",
    "    # convert 'follower_count' column from numerical abbreviation to string numeric form\n",
    "    .withColumn(\"follower_count\", convert_numeric_abb_to_str_numeric_representation(F.col(\"follower_count\")))\n",
    "\n",
    "    # change datatype of 'follower_count' to int\n",
    "    .withColumn(\"follower_count\", F.col(\"follower_count\").cast(\"int\"))\n",
    "\n",
    "    # modify 'save_location' column to only contain the save location path\n",
    "    .withColumn(\"save_location\",\n",
    "                F.col(\"save_location\").substr(\n",
    "                    F.locate(substr=\"/\", str=\"save_location\", pos=1),\n",
    "                    F.length(F.col(\"save_location\"))\n",
    "                ))\n",
    "    \n",
    "    # rename 'index' column to ind\n",
    "    .withColumnRenamed(\"index\", \"ind\")\n",
    ")\n",
    "\n",
    "# write the stream to a delta table\n",
    "query = (\n",
    "    transformed_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpointPath)\n",
    "    .start(outputPath)\n",
    ")\n",
    "\n",
    "# keep the stream running\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "kinesis_pin_data_stream_processing",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
